## Chapter 1 Computer System Overview (Basic Elements)

Everything outside the processor is managed by the OS. 

##### "Basic Elements" includes:
- **Processor**
	- Controls the operation of the computer
	- Performs the data processing functions
	- Referred to as the Central Processing Unit (CPU)
- **I/O Modules**
	- Peripherals! Everything we connect to the computer! 
	- Moves data between computer and external environments 
	- Includes Storage (e.g. HDD), communications equipment, terminals
- **Main Memory** (aka "real memory", "primary memory", aka "RAM")
	- Stores programs (instructions from applications and OS) & data
	- *Volatile*
- **System Bus**
	- Provides for communication among processors, main memory, and I/O modules.

A register is simply a piece of memory within the CPU.

## Processor:
##### Instruction Execution
A program is merely a set of instructions.
1. Processor fetches instructions from memory
	- Increment program counter (PC) 
		- Except for certain cases (loops, jumps, etc.)
	- Load instruction into *instruction register*.
2. Processor executes each instruction

**Instruction Register** holds the fetched instruction. Processor interprets the instruction and performs required action:
- Processor-memory transfer
- Processor-I/O transfer
- Data processing (arithmetic or logical operation)
- Control (conditions, jump, loops etc.)

In our generic architectures, there will be only be one instruction register and program counter

Inside registers may look something like:
Instruction format: 0-3: Opcode, 4-15: Address
Integer format: 0: Sign 1-15: Magnitude

Note that AC often represents accumulator

### Interrupts
- Interrupt normal sequencing of the processor
- Provided to *improve processor utilization*
	- Most I/O devices are slower than the processor
	- Without interrupts, the processor must pause to wait for device
	- This prevents wasteful use of the processor (it just waiting for something slow)
Some **common classes of interrupts**:
- **Program** (e.g. arithmetic overflow, division by zero, attempt to execute an illegal machine instruction)
	- some condition that occurs as a result of an instruction
- **Timer**:
	- Generated by a timer *within the processor* this allows OS to perform certain functions on a regular basis (e.g. updating time stamps, periodic maintenance, etc.)
- **I/O**
	- Generated by an I/O controller.
	- Often to signal normal completion of an operation OR to signal an error.
- **Hardware Failure** (e.g. power failure or memory parity error)

With interrupts, there is a "check for interrupt" stage in the Fetch-Execute cycle at the end. This may lead to executing an ***interrupt-handler routine*** (which is typically part of the OS). 

Note that in the textbook example (Figure 1.5c) the I/O command that the user program isn't waiting for, takes so long, that when another "WRITE" command must be executed before the interrupt handler is received, (the program is trying to write *again* to an I/O device that's still workin' on something) the program will likely "hang" and wait for the response (to then go to the interrupt handler and finish up). This happens when the I/O wait is long.

##### Simple Interrupt Processing
   Hardware:
1. Device Controller issues an interrupt signal to the processor
2. Processor finishes execution of the current instruction before responding to the interrupt
3. Processor tests for a pending interrupt request; sends acknowledgement signal to device that issues the interrupt. The acknowledgement allows the device to stop the signal
4. Processor must prepare to transfer control to interrupt routine. It pushes **PSW** and location of next instruction to be executed (PC) and any other necessary information (hardware dependent) to the control stack.
5. Processor loads new PC value based on interrupt. Depending on architecture and OS design, may be a single program, one for each interrupt, or one for each device and each interrupt.
Software:
6. Save the remainder of program state (such as registers). These are typically saved to the stack. Stack pointer is updated of course.
7. Process the interrupt (go through **interrupt service routine** (**ISR**)). May include examination of status of I/O device, or sending commands/acknowledgments to I/O device.
8. When complete, saved register values are retrieved from the stack and restored to registers.
9. Restore old PSW and PC.

**PSW** (Program Status Word) - a word of bits (current state of interrupted program, condition codes, kernel/user mode bit)

Its important we save all necessary state of the processor because the occurrence of an interrupt is unpredictable.

##### Multiple Interrupts?
e.g. An interrupt occurs while another interrupt is being processed
Two approaches:
1. **Sequential:**  *Disable interrupts* while an interrupt is being processed
2. **Nested**: Use a **priority scheme**. Many things can define the priority

## Main Memory:
Trade off between: *amount* (capacity), *speed* (access time) and *cost*
#### Memory Hierarchy:
Inboard memory:
1. Registers
2. Cache
3. Main Memory
Outboard Storage:
4. Magnetic Disk
Offline storage:
5. Magnetic Tape

Going down the hierarchy, we get:
1. Less cost per bit
2. Increasing capacity
3. increasing access time
4. Decreasing frequency of access to the memory by the processor ("Principle of Locality")

**Cache** (L1) is quicker and smaller than **main memory** (L2).
H = **Hit Ratio** = fraction of all memory accesses that are found in faster memory (L1)
If CPU needs data, finds it in cache, we have a "hit". If we need to go to main memory, its a "miss"

$(H)T_1 + (1-H)(T_1 + T_2)$
Say,
$T_1 = 0.1\mu s$
$T_2 = 1\mu s$
For $H=95\%$, average time to access a byte is $0.15\mu s$

**Principle of Locality**:
Memory references by processor tend to cluster,
Sequential execution: Usually next instruction to be fetched immediately follows last instruction fetched.
Loops: repeat same instructions (cluster)
Table/array processing: next fetch is "beside" last fetch (cluster)

2 Types of Locality:
- **Spatial**: tend to use instruction right after the one you just used
- **Temporal**: tendency to access memory locations that have been used recently

### Secondary Memory
AKA "auxiliary memory"
- external to the main board
- Non-volatile (won't vanish on power off)
- Used to extend Main Memory.
**Cache Memory**:
- Invisible to the OS (but principles similar to virtual memory (ch. 8))
- Processor must access memory at least once per instruction cycle, thus processor execution is limited by memory cycle time
- Exploit the principle of locality with a small, fast memory (cache) between processor and main memory.
- Principles of cache:
	- Contains a copy of a portion of main memory
	- Processor first checks cache
	- If not found, a block of memory is copied into cache
	- Because of **locality of reference**, it is likely that many of the future memory references will be to other bytes in the block. This is why when the CPU requests data that doesn't exist in the cache, the *whole* block containing that data is brought over from Main Memory.

Sometimes multiple levels of cache (denoted L1, L2, ..., with L1 being smallest, fastest and closest to CPU)

Cache memory exists in a different structure than main memory
- Line numbers, not addresses
- "Blocks" of memory from RAM become the "block" part of "lines" of cache
- The "lines" consist of "tags" then blocks.

#CBH and figure out relationship between tags & blocks of cache and memory addresses!
Let's say we have $2^n$ words in main memory, and $k$ words in each block. 
$M = \frac{2^n}k$ number of blocks. Cache consists of $C$ slots (aka *lines*) of $k$ words. $C << M$ (much fewer slots in cache than blocks in memory)
- Each slot contains a **tag** that identifies which block of RAM is being stored. The tag is usually some number of higher-order bits of the addresses.
- When the processor needs to read from RAM, it generates the **Read Address** (RA) of the word to be read.
- The "**Cache read operation**" is simply, 1. receive RA, 2. is it in cache? if so deliver to CPU, otherwise: 3. get the block containing RA from RAM, allocate cache memory, load it and deliver to CPU. 

Main Categories of Cache Design:
- Cache size
- Block Size
- Mapping Function
	- Determines which cache location the block will occupy
	- Two constraints:
		- One block is read in, another may have to be replaced
		- The more flexible the mapping function, the more complex is the circuitry required to search the cache.
- Replacement Algorithm
	- Least Recently Used (LRU) algorithm
		- Replace the block that has been in the cache the longest with no references to it
		- Hardware mechanisms required here.
- Write Policy
	- Dictates when the memory write operation takes place.
- Number of cache level

#CBH Finish the chapter 1 slideshow!